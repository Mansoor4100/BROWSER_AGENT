ğŸ“„ RAG-Based Document Question Answering System
<p align="center"> <img src="https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/langchain_stack.png" width="120" alt="RAG Logo"/> </p> <p align="center"> <b>An end-to-end Retrieval-Augmented Generation (RAG) system for answering questions strictly from uploaded documents.</b> </p> <p align="center"> Built with <b>FastAPI</b>, <b>FAISS</b>, <b>HuggingFace LLMs</b>, and <b>Streamlit</b> â€” runs fully on CPU. </p>
ğŸš€ Key Features

ğŸ“‚ Upload multiple PDF documents

ğŸ” Semantic search using FAISS vector database

ğŸ¤– Context-aware answers using FLAN-T5

âŒ No hallucinations â€” answers only from documents

ğŸ§  Sentence-transformer based embeddings

ğŸ–¥ï¸ Clean Streamlit UI

âš™ï¸ FastAPI backend with modular RAG pipeline

ğŸ’» CPU-only execution (no GPU required)

ğŸ—ï¸ Tech Stack
Layer	Technology
Backend	FastAPI
Frontend	Streamlit
LLM	google/flan-t5-large (HuggingFace)
Embeddings	sentence-transformers/all-MiniLM-L6-v2
Vector DB	FAISS
PDF Loader	PyPDFLoader
Language	Python 3.10+
ğŸ“ Project Structure
rag-doc-qa/
â”‚
â”œâ”€â”€ back/
â”‚   â”œâ”€â”€ app.py                # FastAPI backend
â”‚   â”œâ”€â”€ rag_pipeline.py       # Retrieval + Generation logic
â”‚   â”œâ”€â”€ ingest.py             # PDF ingestion & FAISS indexing
â”‚   â”œâ”€â”€ faiss_index/          # Vector store
â”‚   â””â”€â”€ uploads/              # Uploaded PDFs
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit_app.py      # Streamlit UI
â”‚
â”œâ”€â”€ venv/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/rag-doc-qa.git
cd rag-doc-qa

2ï¸âƒ£ Create Virtual Environment
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

â–¶ï¸ Running the Application
ğŸ”¹ Start Backend (FastAPI)
cd back
uvicorn app:app --reload


Backend available at:

http://127.0.0.1:8000

ğŸ”¹ Start Frontend (Streamlit)
cd frontend
streamlit run streamlit_app.py


Frontend available at:

http://localhost:8501

ğŸ§ª How It Works

User uploads one or more PDF files

PDFs are split into semantic chunks

Chunks are embedded using sentence-transformers

Embeddings are stored in FAISS

User asks a question

Relevant chunks are retrieved

LLM generates an answer only from retrieved context

ğŸ›¡ï¸ Hallucination Control

The system is designed to avoid hallucinations by:

Using strict prompt instructions

Restricting answers to retrieved chunks only

Returning â€œI donâ€™t knowâ€ when context is missing

No external knowledge injection

ğŸ“¸ Demo (Add to GitHub)

You can include:

Screenshots of Streamlit UI

PDF upload flow

Question â†’ Answer output

Optional demo GIF

Example:

![Demo](demo.gif)

ğŸ“Œ Future Improvements

Source citations with page numbers

Chat history & conversational memory

React / Next.js frontend

Dockerized deployment

Cloud hosting (HF Spaces / AWS / Render)

RAG evaluation metrics

Multi-document comparison

ğŸ‘¨â€ğŸ’» Author

Shaik Nabi Mansoor
AI | Machine Learning | Agentic Systems | Full-Stack Development

â­ Why This Project Matters

This project demonstrates:

Real-world RAG architecture

Strong ML + backend integration

Practical handling of LLM limitations

Clean, scalable, production-ready design

Recruiter-relevant AI system building
